<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">
  <head>
    <meta charset="utf-8">
    
    <title>【论文分享】| Medusa: Accelerating Serverless LLM Inference with Materialization (ASPLOS '25) | 飞桨开源社区博客</title>
    <meta name="description" content="Wonderful stories from PaddlePaddle contributors">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/assets/style.BId5x853.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.Cce2DscU.js"></script>
    <link rel="modulepreload" href="/assets/chunks/theme.DU_EuCD6.js">
    <link rel="modulepreload" href="/assets/chunks/framework.BRsttz9t.js">
    <link rel="modulepreload" href="/assets/posts_medusa.md.CafSgstt.lean.js">
    <link rel="icon" type="image/x-icon" href="/favicon.ico">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="theme-color" content="#ffffff">
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-180x180.png">
    <link rel="mask-icon" href="/icons/mask-icon.svg" color="#ffffff">
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-7XR50K1YRK"></script>
    <script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7XR50K1YRK");</script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
    <link rel="manifest" href="/manifest.webmanifest">
  </head>
  <body>
    <div id="app"><div class="antialiased dark:bg-neutral-900 min-h-screen"><div class="max-w-3xl mx-auto px-4 sm:px-6 xl:max-w-5xl xl:px-0"><nav class="flex justify-between items-center py-10 font-bold"><a class="text-xl" href="/" aria-label="飞桨开源社区博客"><img class="inline-block mr-2" style="width:120px;" alt="logo" src="/logo.png"><span class="hidden md:inline dark:text-white">飞桨开源社区博客</span></a><div class="text-sm text-gray-500 dark:text-white leading-5"><a class="hover:text-gray-700 dark:hover:text-gray-200" href="https://github.com/PFCCLab/blog" target="_blank" rel="noopener"><span class="hidden sm:inline">GitHub </span>Source</a><span class="mr-2 ml-2">·</span><a class="hover:text-gray-700 dark:hover:text-gray-200" href="/about.html" rel="noopener">About</a><span class="mr-2 ml-2">·</span><a class="hover:text-gray-700 dark:hover:text-gray-200" href="https://github.com/PFCCLab" target="_blank" rel="noopener">PFCCLab →</a></div></nav></div><main class="max-w-3xl mx-auto px-4 sm:px-6 xl:max-w-5xl xl:px-0"><article class="xl:divide-y xl:divide-gray-200 dark:xl:divide-slate-200/5"><header class="pt-6 xl:pb-10 space-y-1 text-center"><dl><dt class="sr-only">Published on</dt><dd class="text-base leading-6 font-medium text-gray-500 dark:text-gray-300"><time datetime="2025-09-18T12:00:00.000Z">2025年9月18日</time></dd></dl><h1 class="text-3xl leading-9 font-extrabold text-gray-900 dark:text-white tracking-tight sm:text-4xl sm:leading-10 md:text-5xl md:leading-14">【论文分享】| Medusa: Accelerating Serverless LLM Inference with Materialization (ASPLOS &#39;25)</h1></header><div class="divide-y xl:divide-y-0 divide-gray-200 dark:divide-slate-200/5 xl:grid xl:grid-cols-4 xl:gap-x-10 pb-16 xl:pb-20" style="grid-template-rows:auto 1fr;"><dl class="pt-6 pb-10 xl:pt-11 xl:border-b xl:border-gray-200 dark:xl:border-slate-200/5"><dt class="sr-only">Authors</dt><dd><ul class="flex flex-col pl-10 gap-y-5 md:grid md:grid-cols-2 md:gap-x-8 md:gap-y-6 md:pl-0 lg:grid-cols-3 xl:block xl:space-y-8"><!--[--><li class="flex items-center space-x-2"><img src="https://github.com/Siritao.png" alt="author image" class="w-10 h-10 rounded-full"><dl class="text-sm font-medium leading-5 whitespace-nowrap"><dt class="sr-only">Name</dt><dd class="text-gray-900 dark:text-white">陶思睿</dd><dt class="sr-only">GitHub</dt><dd><a href="https://github.com/Siritao" target="_blank" rel="noopnener noreferrer" class="link">@Siritao</a></dd></dl></li><!--]--></ul></dd></dl><div class="divide-y divide-gray-200 dark:divide-slate-200/5 xl:pb-0 xl:col-span-3 xl:row-span-2"><div style="position:relative;" class="prose dark:prose-invert max-w-none pt-10 pb-8"><div><p>本工作旨在解决<strong>Serverless LLM 推理中的冷启动（Cold Start）问题</strong>。冷启动延迟严重影响了用户体验的关键指标——首令牌时间（Time-To-First-Token, TTFT）。</p><h2 id="摘要" tabindex="-1">摘要 <a class="header-anchor" href="#摘要" aria-label="Permalink to &quot;摘要&quot;">​</a></h2><ul><li><strong>核心问题发现:</strong> 作者通过剖析发现，在冷启动的“加载阶段”（Loading Phase），<strong>KV 缓存（KV Cache）初始化</strong>和<strong>CUDA 图（CUDA Graph）捕获</strong>这两个阶段合计占据了高达 50%的时间，是主要的性能瓶颈。</li><li><strong>传统方案的不足:</strong> 现有工作大多专注于消除运行时初始化（如容器、语言环境）的开销，或仅优化模型权重加载，却忽略了 LLM 推理特有的这两个昂贵阶段。</li><li><strong>MEDUSA 的核心思想:</strong> 提出 <strong>“状态物化”（State Materialization）</strong> 方法。即在离线阶段预先计算（物化）KV 缓存的大小和 CUDA 图的结构，在冷启动的在线阶段直接<strong>恢复</strong>这些状态，从而避免了在运行时进行动态性能剖析（Profiling）和图构建，极大缩短了冷启动时间。</li><li><strong>主要贡献:</strong><ol><li>首次明确指出并量化了 KV 缓存初始化和 CUDA 图捕获是 Serverless LLM 冷启动的主要开销。</li><li>提出了创新的、轻量级的 CUDA 图物化与恢复机制，解决了数据指针不确定性和内核地址隐藏性两大挑战。</li><li>实现了 MEDUSA 系统，并在 10 个主流 LLM 模型上验证了其有效性，将加载阶段延迟降低了 42.5%，将真实负载下的 TTFT 尾延迟降低了 53.0%。</li></ol></li></ul><hr><h2 id="背景" tabindex="-1">背景 <a class="header-anchor" href="#背景" aria-label="Permalink to &quot;背景&quot;">​</a></h2><h3 id="serverless-llm-冷启动分解" tabindex="-1">Serverless LLM 冷启动分解 <a class="header-anchor" href="#serverless-llm-冷启动分解" aria-label="Permalink to &quot;Serverless LLM 冷启动分解&quot;">​</a></h3><ul><li><p><strong>冷启动时间线（以 Qwen1.5 4B 为例）</strong><img src="/assets/image-1.DypHe5Qw.png" alt="img1"></p><ul><li><strong>运行时初始化 (22%):</strong> 加载 Python 代码、包、设置执行环境。</li><li><strong>加载阶段 (76%):</strong> 加载模型特定数据并进行预热剖析。这是当前的主要瓶颈，也是 MEDUSA 的优化目标。</li><li><strong>生成首个令牌 (2%):</strong> 实际推理时间。</li></ul></li><li><p><strong>加载阶段深入分解</strong><img src="/assets/image-2.Bek1YSQm.png" alt="img2"></p><ol><li><strong>模型结构初始化:</strong> 实例化模型层、内核，在 GPU 上为模型权重分配张量缓冲区。</li><li><strong>模型权重加载:</strong> 从存储介质加载权重到 GPU 预分配的张量中。</li><li><strong>分词器加载:</strong> 加载模型的分词器。</li><li><strong>KV 缓存初始化 (18%):</strong> 为了有效管理 GPU 内存，框架需要知道有多少空闲内存可用于 KV 缓存。<strong>方法是通过一次“剖析前向”（profiling forward）</strong>：以最大序列长度和最大批次大小运行一次模型前向传播，然后剖析剩余的空闲 GPU 内存空间。这个过程很耗时。</li><li><strong>CUDA 图捕获 (32%):</strong> 为了减少 CPU 内核启动开销，需要为不同的批次大小构建 CUDA 图。<strong>方法是通过“预热”和“捕获前向”</strong> 来构建图，这个过程需要重复多次（例如 vLLM 默认支持 35 种批次大小），非常耗时。</li></ol></li></ul><h3 id="cuda-图的重要性与开销" tabindex="-1">CUDA 图的重要性与开销 <a class="header-anchor" href="#cuda-图的重要性与开销" aria-label="Permalink to &quot;CUDA 图的重要性与开销&quot;">​</a></h3><ul><li><p><strong>为什么需要 CUDA 图？</strong></p><ul><li>GPU 内核执行速度极快（微秒级），CPU 逐个启动内核的开销变得不可忽视。</li><li>CUDA 图将多个内核及其依赖关系组合成一个图，允许通过<strong>一次 CPU 启动</strong>来执行整个图，大幅降低开销。</li><li>实验表明，使用 CUDA 图可获得高达<strong>2.4 倍的性能加速</strong>。因此，不能简单地移除它。</li></ul></li><li><p><strong>CUDA 图捕获的开销来源:</strong></p><ol><li><strong>预热（Warm-up）:</strong> 首次运行某些内核（如 cuBLAS）会触发内部初始化（如同步），这些 API 在捕获期间是被禁止的，因此需要一次额外的预热前向。</li><li><strong>多批次大小:</strong> CUDA 图与特定的批次大小绑定。不同批次大小需要不同的图，vLLM 默认捕获 35 个图，加剧了延迟。</li></ol></li></ul><h3 id="现有解决方案的局限性" tabindex="-1">现有解决方案的局限性 <a class="header-anchor" href="#现有解决方案的局限性" aria-label="Permalink to &quot;现有解决方案的局限性&quot;">​</a></h3><ul><li><strong>热备实例（Hot Spares）:</strong> 资源浪费，且难以覆盖所有模型类型，成本高昂。</li><li><strong>阶段异步执行:</strong> 由于阶段间存在依赖关系（如捕获阶段依赖 KV 缓存），无法完全并行化。权重加载的 I/O 操作也无法完全覆盖其他阶段的延迟。</li><li><strong>延迟捕获:</strong> 将捕获过程推迟到处理请求时进行，并没有消除开销，只是将延迟分摊到了用户请求中，损害了 TTFT。</li></ul><h2 id="medusa-核心设计" tabindex="-1">MEDUSA 核心设计 <a class="header-anchor" href="#medusa-核心设计" aria-label="Permalink to &quot;MEDUSA 核心设计&quot;">​</a></h2><h3 id="总体思路与挑战" tabindex="-1">总体思路与挑战 <a class="header-anchor" href="#总体思路与挑战" aria-label="Permalink to &quot;总体思路与挑战&quot;">​</a></h3><p><strong>思路:</strong> 将运行时（在线）的动态剖析和构建操作，转变为离线物化+在线恢复。</p><ul><li><strong>KV 缓存物化:</strong> 离线运行一次剖析前向，记录下可用空闲 GPU 内存大小。在线冷启动时直接使用该值。</li><li><strong>CUDA 图物化:</strong> 离线捕获 CUDA 图，但并非直接保存二进制图（因为无效），而是保存其“蓝图”（间接索引指针表+内核名称表+CUDA 图的拓扑结构）。在线阶段根据“蓝图”快速重建出可执行的 CUDA 图。</li></ul><p><strong>挑战 I: 数据指针的非确定性</strong></p><ul><li>CUDA 图节点参数中包含了指向输入/输出缓冲区的<strong>数据指针（地址）</strong>。<code>cudaMalloc</code>在不同次启动中返回的地址是随机的。直接保存离线阶段的指针地址，在线恢复时会导致非法内存访问。</li></ul><p><strong>挑战 II: 内核地址的随机性与隐藏性</strong></p><ul><li>CUDA 图节点中记录了<strong>内核函数的地址</strong>。进程的地址空间布局随机化（ASLR）导致每次启动的内核地址都不同。</li><li><strong>更棘手的问题:</strong> 并非所有内核（如 cuBLAS 中的某些内核）都暴露在动态链接库的符号表中，无法通过<code>dlsym</code>等函数按名称查找地址。</li></ul><h3 id="系统概述" tabindex="-1">系统概述 <a class="header-anchor" href="#系统概述" aria-label="Permalink to &quot;系统概述&quot;">​</a></h3><p><img src="/assets/image-3.DEquenIn.png" alt="img3"></p><p>MEDUSA 分为两个阶段：</p><ol><li><p><strong>离线阶段（一次性的）:</strong></p><ul><li><strong>捕获阶段:</strong> 运行标准的冷启动过程，但会拦截所有缓冲区分配调用和内核启动调用，记录序列。同时捕获 CUDA 图并保存其结构信息（节点、依赖关系），并物化 KV 缓存信息。</li><li><strong>分析阶段:</strong> 分析捕获的数据，构建两个关键映射表： <ul><li><strong>间接索引指针表:</strong> 将数据指针参数映射到其对应的缓冲区分配序列中的索引。</li><li><strong>内核名称表:</strong> 记录每个节点对应的内核的（mangled）名称及其所在模块。</li></ul></li></ul></li><li><p><strong>在线阶段（每次冷启动）:</strong></p><ul><li>按正常顺序进行模型结构初始化、权重加载等。</li><li><strong>恢复 KV 缓存:</strong> 直接使用物化的空闲内存值，省去了剖析前向。</li><li><strong>恢复 CUDA 图:</strong><ul><li><strong>重放缓冲区分配序列:</strong> 按记录的顺序重新分配缓冲区，并记录第 i 次分配返回的地址。</li><li><strong>恢复参数指针:</strong> 根据间接索引指针表，将图节点中的参数替换为当前分配序列中对应索引的实际地址。</li><li><strong>恢复内核地址:</strong> 使用触发内核加载所需模块，然后根据内核名称表遍历模块内的所有内核，通过名称匹配找到正确的地址填入图节点。</li></ul></li><li>依赖关系等信息直接用离线保存的元数据恢复。</li></ul></li></ol><h3 id="关键技术细节" tabindex="-1">关键技术细节 <a class="header-anchor" href="#关键技术细节" aria-label="Permalink to &quot;关键技术细节&quot;">​</a></h3><h4 id="参数恢复-间接索引指针" tabindex="-1">参数恢复：间接索引指针 <a class="header-anchor" href="#参数恢复-间接索引指针" aria-label="Permalink to &quot;参数恢复：间接索引指针&quot;">​</a></h4><ul><li><strong>关键观察:</strong> 控制流是确定的！缓冲区分配的顺序（模型结构初始化决定）和内核启动顺序每次运行都严格一致。</li><li><strong>解决方案:</strong><ul><li><strong>离线:</strong> 拦截所有<code>cudaMalloc</code>和<code>cudaFree</code>调用，记录分配/释放序列。对于每个数据指针，在分配序列中<strong>向后追溯</strong>（考虑释放操作），找到其<strong>唯一对应的分配调用索引</strong>。将这个索引<code>i</code>（即间接索引指针）而不是地址本身保存下来。</li><li><strong>在线:</strong> 按相同顺序重放缓冲区分配序列。对于需要恢复的指针，根据其索引<code>i</code>，从当前分配序列中取出第<code>i</code>次分配的实际地址，填入 CUDA 图节点。</li></ul></li><li><strong>缓冲区内容恢复:</strong> 大部分缓冲区内容（如模型权重）无需额外保存和恢复，因为它们会由正常的模型加载过程填充。仅需处理极少数包含魔数的永久性缓冲区，开销极小。</li></ul><h4 id="内核地址恢复-触发内核" tabindex="-1">内核地址恢复：触发内核 <a class="header-anchor" href="#内核地址恢复-触发内核" aria-label="Permalink to &quot;内核地址恢复：触发内核&quot;">​</a></h4><ul><li><strong>对于有符号的内核 (69.2%):</strong> 在线阶段可直接通过<code>dlopen</code>和<code>dlsym</code>按名称查找地址。</li><li><strong>对于无符号的隐藏内核 (如 cuBLAS):</strong><ul><li><strong>方法:</strong> 通过 CUDA 驱动接口<code>cuModuleEnumerateFunctions</code>和<code>cuFuncGetName</code>遍历已加载模块中的所有内核，通过名称匹配来查找地址。</li><li><strong>问题:</strong> 如何确保所需模块已被加载？</li><li><strong>解决方案：触发内核（Triggering-Kernels）</strong><ul><li><strong>初始方案:</strong> 手动寻找能触发目标模块加载的内核（如某种矩阵乘法）。</li><li><strong>优化方案:</strong> 利用 LLM 模型层结构重复的特点。<strong>捕获并运行第一层的 CUDA 图</strong>。由于第一层包含了模型所需的所有内核类型，运行它会自动触发所有必要模块的加载。之后即可遍历这些模块，为所有层的 CUDA 图节点解析出正确的地址。第一层的捕获开销远小于捕获整个模型。</li></ul></li></ul></li></ul><h2 id="实验评估" tabindex="-1">实验评估 <a class="header-anchor" href="#实验评估" aria-label="Permalink to &quot;实验评估&quot;">​</a></h2><h3 id="实验设置" tabindex="-1">实验设置 <a class="header-anchor" href="#实验设置" aria-label="Permalink to &quot;实验设置&quot;">​</a></h3><ul><li><strong>模型:</strong> 10 个来自 HuggingFace 的流行模型，包括 Falcon, Llama2, Qwen1.5, Yi 等。</li><li><strong>对比方案:</strong><ul><li><strong>vLLM:</strong> 基线方案。</li><li><strong>vLLM + async:</strong> 基线 + 模型权重加载异步化。</li><li><strong>MEDUSA:</strong> 本文方案，包含 CUDA 图和 KV 缓存物化。</li></ul></li><li><strong>评估指标:</strong> 加载阶段延迟、总冷启动延迟、真实负载下的 TTFT 尾延迟。</li></ul><h3 id="主要结果" tabindex="-1">主要结果 <a class="header-anchor" href="#主要结果" aria-label="Permalink to &quot;主要结果&quot;">​</a></h3><p><img src="/assets/image-4.C6iYTKhn.png" alt="img4"></p><ul><li><strong>加载阶段延迟 (a):</strong> MEDUSA 平均比 vLLM 降低<strong>42.5%</strong>，比 vLLM+async 降低<strong>34.4%</strong>。证明了物化方法的显著优势。</li><li><strong>总体冷启动延迟 (b):</strong> 平均降低<strong>34.9%</strong>。</li></ul><p><img src="/assets/image-5.6QxnjsMn.png" alt="img5"></p><ul><li><strong>分解分析 :</strong> 以 Qwen1.5 4B 为例，MEDUSA 将 KV 缓存初始化时间从 0.50s 降至 0.02s，将 CUDA 图捕获时间从 0.90s 降至 0.57s。并通过异步执行优化，进一步消除了气泡。</li></ul><p><img src="/assets/image-6.Cj2isEL1.png" alt="img6"></p><ul><li><strong>离线开销:</strong> 离线阶段平均耗时约 39.2 秒（捕获 9.7s + 分析 29.5s），对于一次性的操作来说是可接受的。</li><li><strong>真实负载测试 (ShareGPT 轨迹):</strong><ul><li>在 Poisson 请求分布下，MEDUSA 显著降低了<strong>99th 百分位的 TTFT 尾延迟</strong>（最高降低 53.0%）。</li><li>在不同 RPS（每秒请求数）和系统吞吐量下，MEDUSA 始终能提供更低、更稳定的尾延迟。</li><li>与“完全移除 CUDA 图”（w/o CUDA Graph）的方案相比，MEDUSA 表现更好，因为它既降低了冷启动延迟，又保留了 CUDA 图对推理吞吐量的加速 benefits。</li></ul></li></ul><h2 id="结论与展望" tabindex="-1">结论与展望 <a class="header-anchor" href="#结论与展望" aria-label="Permalink to &quot;结论与展望&quot;">​</a></h2><ul><li><strong>总结:</strong> MEDUSA 通过状态物化有效解决了 Serverless LLM 推理冷启动中的核心瓶颈。其设计的间接索引指针和触发内核方法巧妙地克服了 CUDA 图物化的底层挑战。</li><li><strong>意义:</strong> 这项工作不仅是一个高效的系统实现，更重要的是为优化类似系统（凡依赖动态剖析和构建的系统）提供了新颖的设计思路。</li></ul></div></div></div><footer class="text-sm font-medium leading-5 divide-y divide-gray-200 dark:divide-slate-200/5 xl:col-start-1 xl:row-start-2"><div class="py-8"><h2 class="text-xs tracking-wide uppercase text-gray-500 dark:text-white"> Next Article </h2><div class="link"><a href="/posts/zju-se-os">飞桨进浙江大学软件学院：一场浙大限定的开源大冒险 🚀</a></div></div><div class="py-8"><h2 class="text-xs tracking-wide uppercase text-gray-500 dark:text-white"> Previous Article </h2><div class="link"><a href="/posts/post-training-overview">Post-training of LLM（产品经理民科普及版）</a></div></div><div class="pt-8"><a class="link" href="/">← Back to the blog</a></div></footer></div></article></main></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"about.md\":\"CzQAAf4V\",\"index.md\":\"AYBfx0e1\",\"pages_1.md\":\"wccGEy_C\",\"pages_2.md\":\"CNyCU79V\",\"pages_3.md\":\"B2EUOUN_\",\"pages_4.md\":\"JDLWzQTr\",\"pages_5.md\":\"wVTJN5qg\",\"pages_6.md\":\"ajK8Xo7n\",\"pages_7.md\":\"D1V8X6Zv\",\"pages_8.md\":\"C9fPTFNT\",\"posts_2023-os-report.md\":\"KoqxymVZ\",\"posts_2024-summary.md\":\"BFc6O-qh\",\"posts_2025-3rd-sw-conf.md\":\"oYRo7W4-\",\"posts_2025wuzhen.md\":\"Dc_dInmo\",\"posts_attention-sharing-sirui.md\":\"CZuKocbE\",\"posts_attnsink-paper-sharing.md\":\"RrNa11ke\",\"posts_buaa-starter.md\":\"DZMcxZ08\",\"posts_build-ernie-websearch.md\":\"NXDkfuT1\",\"posts_ccf-pku.md\":\"DBv8vhzc\",\"posts_chengdu-kaiyuanshe.md\":\"BuU5nIAV\",\"posts_chuan-story.md\":\"UiGi40mG\",\"posts_context-parrel-sharing.md\":\"BIHMolvU\",\"posts_cosyvoice.md\":\"BzDq48dR\",\"posts_decode-sparse-attention-method.md\":\"Ck9FTHi3\",\"posts_deepseek-tech-visualized.md\":\"BKbTJj_v\",\"posts_ernie45-paddleocr-case.md\":\"RR0g2AWK\",\"posts_first-post.md\":\"DfLGbfoW\",\"posts_flashoverlap-paper-sharing.md\":\"DxZQIYNn\",\"posts_flux-paper-sharing.md\":\"BI5_0SX0\",\"posts_glcc-luqi.md\":\"DiK5hFCr\",\"posts_hackathon-5th-episode01.md\":\"Df0O2ch6\",\"posts_hackathon-5th-episode02.md\":\"DljLYSrV\",\"posts_hackathon-5th-episode03.md\":\"B8-h4PLM\",\"posts_hackathon-6th-summary.md\":\"CjII45CK\",\"posts_hackathon-7th-summary.md\":\"BZoGEYXn\",\"posts_high-precision-rag-system.md\":\"CfUn4ZYF\",\"posts_huanggua-story.md\":\"3fmStkaX\",\"posts_huangjiyi-story.md\":\"DnuzI-cZ\",\"posts_ijcai-2024-competition.md\":\"B2ssZ-R3\",\"posts_io-paper-sharing.md\":\"DaAgwrgf\",\"posts_japan-hackathon.md\":\"B4r6pLgQ\",\"posts_kv-cache-drop-for-llm-infer.md\":\"FlfjGRC8\",\"posts_kv-cache-overview.md\":\"BLQqMIXC\",\"posts_lightning-talks.md\":\"CkzvQk5M\",\"posts_ligoml-story.md\":\"DwUUSbET\",\"posts_limin-story.md\":\"Cq3bk9v5\",\"posts_loaf-sharing.md\":\"Cb6ZXW4C\",\"posts_lzj-sharing.md\":\"VfbU8rSL\",\"posts_magiattention-sharing.md\":\"9uhQAeai\",\"posts_medusa.md\":\"CafSgstt\",\"posts_megascale-infer-paper-sharing.md\":\"DdCevYFk\",\"posts_netmoe-paper-sharing.md\":\"CLmhz_8_\",\"posts_newcomers-manual.md\":\"2fGdj5Uh\",\"posts_newhardware-2nd-event.md\":\"DP8zlBFH\",\"posts_nknan-story.md\":\"ClIDCktw\",\"posts_outlier-in-llm-paper-sharing.md\":\"DxJIzetH\",\"posts_pactguard-ernie-pp.md\":\"9blIBKHw\",\"posts_paddle-debug-methods.md\":\"BGwMbwTc\",\"posts_paddle-pipeline-parallel.md\":\"BXrwKs5W\",\"posts_paddle-scnu.md\":\"CsOmY_uw\",\"posts_paddleocr-release.md\":\"BW1kPCNf\",\"posts_paddleocr-vl-for-manga.md\":\"Dw-Lu7qS\",\"posts_pfcc-36th.md\":\"7668E08h\",\"posts_pku-course-2025.md\":\"Bo1xw7zs\",\"posts_pku-course.md\":\"B3Zt9sAI\",\"posts_post-training-overview.md\":\"bo23EEX8\",\"posts_pytorch-conference-01.md\":\"B14UyS7F\",\"posts_sanbu-story.md\":\"BVuipAfQ\",\"posts_shanghai-event.md\":\"ApG0YjGd\",\"posts_shun-story.md\":\"Bbt17jzP\",\"posts_starter-camp-5th.md\":\"5mxEq-BX\",\"posts_starter-camp.md\":\"TdAP6itn\",\"posts_suzhou-kaifangyuanzi.md\":\"CCEtzRNL\",\"posts_swagger-deepseek-r1.md\":\"BLYovS4m\",\"posts_tao-story.md\":\"VncVwA3l\",\"posts_type-hints-project.md\":\"BYrNy-i6\",\"posts_vattention-paper-sharing.md\":\"CXLeYUSG\",\"posts_wangxin-story.md\":\"B1ojKWVC\",\"posts_wuxi-kaifangyuanzi.md\":\"DVani9MK\",\"posts_xdoctest-project.md\":\"D1YjEXRk\",\"posts_xdu-xjtu-os.md\":\"CF04UOFz\",\"posts_xian-event.md\":\"DSOIzACM\",\"posts_yanguohao-story.md\":\"Caxzgkds\",\"posts_yinfan-story.md\":\"BQD8IBpP\",\"posts_zhangyiqiao-story.md\":\"D5MUTeFE\",\"posts_zheng-story.md\":\"Brmo3hBv\",\"posts_zju-event.md\":\"CTI-BvwC\",\"posts_zju-se-os.md\":\"D4JHDafp\",\"posts_zuckerberg-letter-post.md\":\"CHMyePoL\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"zh-CN\",\"dir\":\"ltr\",\"title\":\"飞桨开源社区博客\",\"description\":\"Wonderful stories from PaddlePaddle contributors\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"postsPerPage\":10},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":true}");</script>
    
  </body>
</html>