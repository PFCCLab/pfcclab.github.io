<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">
  <head>
    <meta charset="utf-8">
    
    <title>【论文分享】| KV 压缩技术综述：高效LLM推理的 KV Cache 优化 | 飞桨开源社区博客</title>
    <meta name="description" content="Wonderful stories from PaddlePaddle contributors">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/assets/style.BId5x853.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.Cce2DscU.js"></script>
    <link rel="modulepreload" href="/assets/chunks/theme.DU_EuCD6.js">
    <link rel="modulepreload" href="/assets/chunks/framework.BRsttz9t.js">
    <link rel="modulepreload" href="/assets/posts_kv-cache-overview.md.BLQqMIXC.lean.js">
    <link rel="icon" type="image/x-icon" href="/favicon.ico">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="theme-color" content="#ffffff">
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-180x180.png">
    <link rel="mask-icon" href="/icons/mask-icon.svg" color="#ffffff">
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-7XR50K1YRK"></script>
    <script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7XR50K1YRK");</script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
    <link rel="manifest" href="/manifest.webmanifest">
  </head>
  <body>
    <div id="app"><div class="antialiased dark:bg-neutral-900 min-h-screen"><div class="max-w-3xl mx-auto px-4 sm:px-6 xl:max-w-5xl xl:px-0"><nav class="flex justify-between items-center py-10 font-bold"><a class="text-xl" href="/" aria-label="飞桨开源社区博客"><img class="inline-block mr-2" style="width:120px;" alt="logo" src="/logo.png"><span class="hidden md:inline dark:text-white">飞桨开源社区博客</span></a><div class="text-sm text-gray-500 dark:text-white leading-5"><a class="hover:text-gray-700 dark:hover:text-gray-200" href="https://github.com/PFCCLab/blog" target="_blank" rel="noopener"><span class="hidden sm:inline">GitHub </span>Source</a><span class="mr-2 ml-2">·</span><a class="hover:text-gray-700 dark:hover:text-gray-200" href="/about.html" rel="noopener">About</a><span class="mr-2 ml-2">·</span><a class="hover:text-gray-700 dark:hover:text-gray-200" href="https://github.com/PFCCLab" target="_blank" rel="noopener">PFCCLab →</a></div></nav></div><main class="max-w-3xl mx-auto px-4 sm:px-6 xl:max-w-5xl xl:px-0"><article class="xl:divide-y xl:divide-gray-200 dark:xl:divide-slate-200/5"><header class="pt-6 xl:pb-10 space-y-1 text-center"><dl><dt class="sr-only">Published on</dt><dd class="text-base leading-6 font-medium text-gray-500 dark:text-gray-300"><time datetime="2025-10-16T12:00:00.000Z">2025年10月16日</time></dd></dl><h1 class="text-3xl leading-9 font-extrabold text-gray-900 dark:text-white tracking-tight sm:text-4xl sm:leading-10 md:text-5xl md:leading-14">【论文分享】| KV 压缩技术综述：高效LLM推理的 KV Cache 优化</h1></header><div class="divide-y xl:divide-y-0 divide-gray-200 dark:divide-slate-200/5 xl:grid xl:grid-cols-4 xl:gap-x-10 pb-16 xl:pb-20" style="grid-template-rows:auto 1fr;"><dl class="pt-6 pb-10 xl:pt-11 xl:border-b xl:border-gray-200 dark:xl:border-slate-200/5"><dt class="sr-only">Authors</dt><dd><ul class="flex flex-col pl-10 gap-y-5 md:grid md:grid-cols-2 md:gap-x-8 md:gap-y-6 md:pl-0 lg:grid-cols-3 xl:block xl:space-y-8"><!--[--><li class="flex items-center space-x-2"><img src="https://github.com/VAthree.png" alt="author image" class="w-10 h-10 rounded-full"><dl class="text-sm font-medium leading-5 whitespace-nowrap"><dt class="sr-only">Name</dt><dd class="text-gray-900 dark:text-white">刘思然</dd><dt class="sr-only">GitHub</dt><dd><a href="https://github.com/VAthree" target="_blank" rel="noopnener noreferrer" class="link">@VAthree</a></dd></dl></li><!--]--></ul></dd></dl><div class="divide-y divide-gray-200 dark:divide-slate-200/5 xl:pb-0 xl:col-span-3 xl:row-span-2"><div style="position:relative;" class="prose dark:prose-invert max-w-none pt-10 pb-8"><div><p>本文梳理近期主流的 KV 压缩/驱逐思路（Prefill vs. Decoding），对比 H2O、PyramidKV、SnapKV、Quest 等代表方法的策略与表现。</p><h2 id="引言与背景" tabindex="-1">引言与背景 <a class="header-anchor" href="#引言与背景" aria-label="Permalink to &quot;引言与背景&quot;">​</a></h2><p>大型语言模型 (Large Language Models, LLMs) 在处理长上下文任务时，面临着严重的内存和计算开销问题。其中，Key-Value (KV) Cache 是 Transformer 架构中自注意力机制的核心组件，用于存储先前 token 的 Key 和 Value 表示，以避免在生成阶段重复计算。然而，随着上下文长度的增加，KV Cache 的内存占用呈线性增长，通常占 GPU 内存的 70% 以上。这限制了模型处理长序列的能力，并导致推理延迟增加。</p><p>KV Cache 压缩/驱逐技术旨在通过识别和保留高重要性的 KV 条目，减少缓存大小，同时尽可能保持模型性能。核心观察是：注意力分数矩阵高度稀疏（通常超过 95%），即大多数 KV 条目对当前查询的贡献很小。这些技术通常分为两类：</p><ul><li><strong>Prefill/Prompt 阶段压缩</strong>：针对输入提示的 KV Cache 进行压缩，减少初始内存占用。</li><li><strong>Decoding 阶段压缩</strong>：在生成过程中动态驱逐或选择 KV 条目，提高效率。</li></ul><p>这些方法利用注意力模式的稀疏性、层级差异和查询相关性，实现内存节省和吞吐提升。根据 KV Cache Compression for Inference Efficiency in LLMs，常见 trade-off 包括压缩率与性能损失的平衡，以及是否需要微调模型。</p><h2 id="代表性工作" tabindex="-1">代表性工作 <a class="header-anchor" href="#代表性工作" aria-label="Permalink to &quot;代表性工作&quot;">​</a></h2><h3 id="_1-h2o-heavy-hitter-oracle-for-efficient-generative-inference-of-large-language-models" tabindex="-1">1. H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models <a class="header-anchor" href="#_1-h2o-heavy-hitter-oracle-for-efficient-generative-inference-of-large-language-models" aria-label="Permalink to &quot;1. H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models&quot;">​</a></h3><ul><li><strong>论文链接</strong>：<a href="https://arxiv.org/abs/2306.14048" target="_blank" rel="noreferrer">https://arxiv.org/abs/2306.14048</a></li><li><strong>针对阶段</strong>：Decoding</li><li><strong>核心思想</strong>：尽管 LLMs 是稠密训练的，但注意力分数矩阵高度稀疏（所有层稀疏度 &gt;95%）。这表明在生成下一个 token 时，无需访问所有历史 KV，注意力模块的稀疏性说明生成下一个 token 时访问此前所有的 key 和 value 的 embedding 是没有必要的，只需保留“重击者”（Heavy Hitters, H₂）——那些贡献大部分注意力分数的 token。</li><li><strong>方法</strong>：将 KV 驱逐建模为动态子模优化问题。驱逐策略：基于累积注意力分数，驱逐分数最低的 token，同时保留最近 token 和部分 H₂ token（例如 20%）。这利用了 token 共现的自然模式，确保驱逐低贡献 token 不会显著影响性能。</li><li><strong>性能</strong>：在 OPT、LLaMA 和 GPT-NeoX 模型上，吞吐提升高达 29x（相比 DeepSpeed Zero-Inference），延迟降低 1.9x。理论上证明了驱逐算法的保证。</li><li><strong>理解</strong>：H2O 是早期 KV 驱逐的典范，强调动态性和稀疏利用。它简单高效，但可能忽略层级差异，导致在复杂长上下文任务中性能波动。适用于长内容生成如对话系统。</li></ul><h3 id="_2-pyramidkv-dynamic-kv-cache-compression-based-on-pyramidal-information-funneling" tabindex="-1">2. PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling <a class="header-anchor" href="#_2-pyramidkv-dynamic-kv-cache-compression-based-on-pyramidal-information-funneling" aria-label="Permalink to &quot;2. PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling&quot;">​</a></h3><ul><li><strong>论文链接</strong>：<a href="https://arxiv.org/abs/2406.02069" target="_blank" rel="noreferrer">https://arxiv.org/abs/2406.02069</a></li><li><strong>针对阶段</strong>：Prefill/Decoding</li><li><strong>核心思想</strong>：传统方法对所有 Transformer 层统一压缩 KV Cache，“一视同仁”地用相同的压缩设置，压缩到同样的长度，忽略了层级注意力模式的差异。研究 Llama 模型进行多文档问答的逐层注意力图，观察到“金字塔型信息汇聚”：底层注意力稠密（均匀分布），中间层局部聚焦，上层极度稀疏（Attention Sink 和 Massive Activation）。结论：<strong>可能在较高层的稀疏注意力中保留许多不重要的 tokens，而忽略较低层密集注意力中的许多重要的 tokens</strong>。</li><li><strong>方法</strong>：层级动态分配 KV 预算——底层分配更多缓存，上层减少。确定预算后，根据注意力分数选择 KV token，始终保留 instruction token（类似窗口模式）。这避免了在高层保留过多无关 token，并在底层保留关键全局信息。</li><li><strong>性能</strong>：在 Llama-3-8B 和 Mistral-7B 上，仅用 2.5% KV Cache 保持 90% 性能。在 LongBench 等任务中，优于基线 20% ACC（size=128 时）。</li><li><strong>理解</strong>：PyramidKV 巧妙利用层级稀疏梯度，提高压缩效率。它比“一视同仁”方法更智能，但计算预算分配可能增加开销。适合多文档 QA 等需要层级信息聚合的任务。</li></ul><h3 id="_3-snapkv-llm-knows-what-you-are-looking-for-before-generation" tabindex="-1">3. SnapKV: LLM Knows What You are Looking for Before Generation <a class="header-anchor" href="#_3-snapkv-llm-knows-what-you-are-looking-for-before-generation" aria-label="Permalink to &quot;3. SnapKV: LLM Knows What You are Looking for Before Generation&quot;">​</a></h3><ul><li><strong>论文链接</strong>：<a href="https://arxiv.org/abs/2404.14469" target="_blank" rel="noreferrer">https://arxiv.org/abs/2404.14469</a></li><li><strong>针对阶段</strong>：Prefill</li><li><strong>核心思想</strong>：在生成过程中，注意力头一致关注提示中的特定位置。这可以通过提示末尾的“观察窗口”（Observation Window）捕捉。</li><li><strong>方法</strong>：两步过程：1) 基于观察窗口（e.g., 最后 16 token）的注意力分布，通过投票算法选出重要 KV 位置（对 softmax 后分数求和，选择 top-k）；2) 将选出 KV（L_prefix）与观察窗口拼接，形成压缩 KV Cache。针对 Prefill 阶段压缩提示 KV。 <ul><li>假设 Prompt 序列长度为 1000，Observation Window 大小为 16，我们希望将 KV 缓存压缩至 256。SnapKV 首先基于最后 16 个 Token 的注意力分布，通过 Voting 算法选出最重要的 240 个位置。然后将这 240 个位置对应的 Key 和 Value 与 Observation Window 拼接，形成大小为 256 的新 KV 缓存。这样,后续的生成过程就只需在显著缩减的 KV 缓存上进行注意力计算，从而大幅提升了效率。</li></ul></li><li><strong>性能</strong>：对于 16K token 输入，生成速度提升 3.6x，内存效率 8.2x。在 16 个长序列数据集上性能相当，可处理 380K 上下文（A100 GPU）。</li><li><strong>理解</strong>：SnapKV 无需微调，简单高效，专注于提示压缩。它假设注意力模式稳定，但可能在动态变化的上下文中（如交互式生成）效果减弱。</li></ul><h3 id="_4-quest-query-aware-sparsity-for-efficient-long-context-llm-inference" tabindex="-1">4. Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference <a class="header-anchor" href="#_4-quest-query-aware-sparsity-for-efficient-long-context-llm-inference" aria-label="Permalink to &quot;4. Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference&quot;">​</a></h3><ul><li><strong>论文链接</strong>：<a href="https://arxiv.org/abs/2406.10774" target="_blank" rel="noreferrer">https://arxiv.org/abs/2406.10774</a></li><li><strong>针对阶段</strong>：Decoding</li><li><strong>核心思想</strong>：token 重要性高度依赖当前查询/最新的 Q（Query）。前两层注意力重要，后层稀疏度高。Quest 针对 Decoding 阶段，不压缩 Prefill KV，而是动态选择相关 KV。</li><li><strong>方法</strong>：块级处理：将 KV Cache 分块（pages），跟踪每个块的 min/max Key 值。与当前 Query 逐元素乘积，计算块分数（sum of max elements），选择 top-k 块进行注意力计算。前两层保持稠密计算。</li><li><strong>性能</strong>：自注意力加速 2.23x，总推理延迟降低 7.03x。在长依赖任务上准确率损失 negligible。</li><li><strong>理解</strong>：Quest 的查询感知性使其在动态生成中出色，避免了静态压缩的局限。但块级计算增加少量开销，早层处理确保基础信息完整。适用于长上下文推理如总结或 QA。</li></ul><h3 id="_5-can-llms-maintain-fundamental-abilities-under-kv-cache-compression" tabindex="-1">5. Can LLMs Maintain Fundamental Abilities under KV Cache Compression? <a class="header-anchor" href="#_5-can-llms-maintain-fundamental-abilities-under-kv-cache-compression" aria-label="Permalink to &quot;5. Can LLMs Maintain Fundamental Abilities under KV Cache Compression?&quot;">​</a></h3><ul><li><strong>论文链接</strong>：<a href="https://arxiv.org/abs/2502.01941" target="_blank" rel="noreferrer">https://arxiv.org/abs/2502.01941</a></li><li><strong>针对阶段</strong>：Prefill/Decoding</li><li><strong>核心思想</strong>：现有压缩方法（如 SnapKV、PyramidKV）在长上下文基准上表现好，但对模型基础能力（如世界知识、常识推理、数学、代码生成、安全和长上下文理解）的影响未充分研究。</li><li><strong>方法</strong>：引入 KVFundaBench 基准，评估压缩对基础能力的任务依赖降解。发现：任务依赖、模型类型鲁棒性、提示长度脆弱性、块级优越性、提示增益敏感性和长上下文生成敏感性。提出 ShotKV 方法：分开处理 Prefill 和 Decoding，保持 shot-level 语义一致。</li><li><strong>性能</strong>：ShotKV 在激进压缩下，长上下文生成提升 9%-18%。比 SnapKV 和 PyramidKV 更鲁棒。</li><li><strong>理解</strong>：这篇更像是评估框架，强调压缩不能牺牲核心能力。揭示了方法局限，如长提示更易降级。建议在设计时考虑多任务鲁棒性。</li></ul><h3 id="_6-chunkkv-semantic-preserving-kv-cache-compression-for-efficient-long-context-llm-inference" tabindex="-1">6. ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference <a class="header-anchor" href="#_6-chunkkv-semantic-preserving-kv-cache-compression-for-efficient-long-context-llm-inference" aria-label="Permalink to &quot;6. ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference&quot;">​</a></h3><ul><li><strong>论文链接</strong>：<a href="https://arxiv.org/abs/2502.00299" target="_blank" rel="noreferrer">https://arxiv.org/abs/2502.00299</a></li><li><strong>针对阶段</strong>：Prefill</li><li><strong>核心思想</strong>：针对 Prefill 阶段，传统 token-level 压缩可能破坏语义。使用语义块（chunks）作为单位，保留完整语言结构。</li><li><strong>方法</strong>：Chunk-based：流程和 snapkv 差不多，基于最后 w token 的 Query 计算 score，按 chunksize c 分块，求和选 top-k 块保存 KV。Decoding 时替换最后的 w KV。Layer-Wise Index Reuse：每 N_reuse 层共享压缩索引，减少计算。（算法：if l % N_reuse == 0，就由这层来计算压缩的 KV 索引，剩下的 N_reuse - 1 层共用相同的 kv 索引。） <ul><li>Target：提高吞吐、小幅度降低精度。</li></ul></li><li><strong>性能</strong>：比 SnapKV、PyramidKV 精度高 8.7%，吞吐提升 26.5%。在 LongBench 等基准上优异。</li><li><strong>理解</strong>：ChunkKV 强调语义保存，避免碎片化上下文。层级复用平衡延迟和吞吐，效果微降。适合需要语义完整性的任务，如长上下文生成。</li></ul><h2 id="技术比较与理解" tabindex="-1">技术比较与理解 <a class="header-anchor" href="#技术比较与理解" aria-label="Permalink to &quot;技术比较与理解&quot;">​</a></h2><p>总结这些技术的关键特征：</p><table tabindex="0"><thead><tr><th>技术</th><th>针对阶段</th><th>核心机制</th><th>压缩率/性能提升</th><th>优势与局限</th></tr></thead><tbody><tr><td>H2O</td><td>Decoding</td><td>注意力分数驱逐 H₂ token</td><td>吞吐 29x，内存线性减少</td><td>简单动态；忽略层级差异</td></tr><tr><td>PyramidKV</td><td>Prefill/Decoding</td><td>层级预算分配 + 注意力选择</td><td>2.5% KV 保持 90% 性能</td><td>层级优化；预算计算开销</td></tr><tr><td>SnapKV</td><td>Prefill</td><td>观察窗口投票 + 拼接</td><td>生成 3.6x，内存 8.2x</td><td>无微调；假设模式稳定</td></tr><tr><td>Quest</td><td>Decoding</td><td>查询感知块选择</td><td>自注意力 2.23x，延迟 7.03x</td><td>动态相关；早层处理复杂</td></tr><tr><td>ShotKV (from eval)</td><td>Prefill/Decoding</td><td>分阶段 + 语义一致</td><td>长生成 9-18%</td><td>鲁棒评估；非独立方法</td></tr><tr><td>ChunkKV</td><td>Prefill</td><td>块级 score + 层复用</td><td>精度 +8.7%，吞吐 +26.5%</td><td>语义保存；共享索引高效</td></tr></tbody></table><p><strong>整体理解</strong>：这些技术共同利用注意力稀疏性，但侧重点不同：H2O 和 Quest 更注重动态驱逐，PyramidKV 和 ChunkKV 强调层级/语义优化，SnapKV 简单高效。挑战在于平衡压缩率与性能，尤其在长上下文下。未来方向可能包括结合量化（如 CacheGen）和分布式缓存（如 ShadowKV）。</p></div></div></div><footer class="text-sm font-medium leading-5 divide-y divide-gray-200 dark:divide-slate-200/5 xl:col-start-1 xl:row-start-2"><div class="py-8"><h2 class="text-xs tracking-wide uppercase text-gray-500 dark:text-white"> Next Article </h2><div class="link"><a href="/posts/lightning-talks">⚡️PaddlePaddle Contributor Lightning Talks 回顾：闪电之上，是开源的热与光</a></div></div><div class="py-8"><h2 class="text-xs tracking-wide uppercase text-gray-500 dark:text-white"> Previous Article </h2><div class="link"><a href="/posts/pku-course-2025">飞桨&amp;文心开源高校行之北京大学</a></div></div><div class="pt-8"><a class="link" href="/">← Back to the blog</a></div></footer></div></article></main></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"about.md\":\"CzQAAf4V\",\"index.md\":\"AYBfx0e1\",\"pages_1.md\":\"wccGEy_C\",\"pages_2.md\":\"CNyCU79V\",\"pages_3.md\":\"B2EUOUN_\",\"pages_4.md\":\"JDLWzQTr\",\"pages_5.md\":\"wVTJN5qg\",\"pages_6.md\":\"ajK8Xo7n\",\"pages_7.md\":\"D1V8X6Zv\",\"pages_8.md\":\"C9fPTFNT\",\"posts_2023-os-report.md\":\"KoqxymVZ\",\"posts_2024-summary.md\":\"BFc6O-qh\",\"posts_2025-3rd-sw-conf.md\":\"oYRo7W4-\",\"posts_2025wuzhen.md\":\"Dc_dInmo\",\"posts_attention-sharing-sirui.md\":\"CZuKocbE\",\"posts_attnsink-paper-sharing.md\":\"RrNa11ke\",\"posts_buaa-starter.md\":\"DZMcxZ08\",\"posts_build-ernie-websearch.md\":\"NXDkfuT1\",\"posts_ccf-pku.md\":\"DBv8vhzc\",\"posts_chengdu-kaiyuanshe.md\":\"BuU5nIAV\",\"posts_chuan-story.md\":\"UiGi40mG\",\"posts_context-parrel-sharing.md\":\"BIHMolvU\",\"posts_cosyvoice.md\":\"BzDq48dR\",\"posts_decode-sparse-attention-method.md\":\"Ck9FTHi3\",\"posts_deepseek-tech-visualized.md\":\"BKbTJj_v\",\"posts_ernie45-paddleocr-case.md\":\"RR0g2AWK\",\"posts_first-post.md\":\"DfLGbfoW\",\"posts_flashoverlap-paper-sharing.md\":\"DxZQIYNn\",\"posts_flux-paper-sharing.md\":\"BI5_0SX0\",\"posts_glcc-luqi.md\":\"DiK5hFCr\",\"posts_hackathon-5th-episode01.md\":\"Df0O2ch6\",\"posts_hackathon-5th-episode02.md\":\"DljLYSrV\",\"posts_hackathon-5th-episode03.md\":\"B8-h4PLM\",\"posts_hackathon-6th-summary.md\":\"CjII45CK\",\"posts_hackathon-7th-summary.md\":\"BZoGEYXn\",\"posts_high-precision-rag-system.md\":\"CfUn4ZYF\",\"posts_huanggua-story.md\":\"3fmStkaX\",\"posts_huangjiyi-story.md\":\"DnuzI-cZ\",\"posts_ijcai-2024-competition.md\":\"B2ssZ-R3\",\"posts_io-paper-sharing.md\":\"DaAgwrgf\",\"posts_japan-hackathon.md\":\"B4r6pLgQ\",\"posts_kv-cache-drop-for-llm-infer.md\":\"FlfjGRC8\",\"posts_kv-cache-overview.md\":\"BLQqMIXC\",\"posts_lightning-talks.md\":\"CkzvQk5M\",\"posts_ligoml-story.md\":\"DwUUSbET\",\"posts_limin-story.md\":\"Cq3bk9v5\",\"posts_loaf-sharing.md\":\"Cb6ZXW4C\",\"posts_lzj-sharing.md\":\"VfbU8rSL\",\"posts_magiattention-sharing.md\":\"9uhQAeai\",\"posts_medusa.md\":\"CafSgstt\",\"posts_megascale-infer-paper-sharing.md\":\"DdCevYFk\",\"posts_netmoe-paper-sharing.md\":\"CLmhz_8_\",\"posts_newcomers-manual.md\":\"2fGdj5Uh\",\"posts_newhardware-2nd-event.md\":\"DP8zlBFH\",\"posts_nknan-story.md\":\"ClIDCktw\",\"posts_outlier-in-llm-paper-sharing.md\":\"DxJIzetH\",\"posts_pactguard-ernie-pp.md\":\"9blIBKHw\",\"posts_paddle-debug-methods.md\":\"BGwMbwTc\",\"posts_paddle-pipeline-parallel.md\":\"BXrwKs5W\",\"posts_paddle-scnu.md\":\"CsOmY_uw\",\"posts_paddleocr-release.md\":\"BW1kPCNf\",\"posts_paddleocr-vl-for-manga.md\":\"Dw-Lu7qS\",\"posts_pfcc-36th.md\":\"7668E08h\",\"posts_pku-course-2025.md\":\"Bo1xw7zs\",\"posts_pku-course.md\":\"B3Zt9sAI\",\"posts_post-training-overview.md\":\"bo23EEX8\",\"posts_pytorch-conference-01.md\":\"B14UyS7F\",\"posts_sanbu-story.md\":\"BVuipAfQ\",\"posts_shanghai-event.md\":\"ApG0YjGd\",\"posts_shun-story.md\":\"Bbt17jzP\",\"posts_starter-camp-5th.md\":\"5mxEq-BX\",\"posts_starter-camp.md\":\"TdAP6itn\",\"posts_suzhou-kaifangyuanzi.md\":\"CCEtzRNL\",\"posts_swagger-deepseek-r1.md\":\"BLYovS4m\",\"posts_tao-story.md\":\"VncVwA3l\",\"posts_type-hints-project.md\":\"BYrNy-i6\",\"posts_vattention-paper-sharing.md\":\"CXLeYUSG\",\"posts_wangxin-story.md\":\"B1ojKWVC\",\"posts_wuxi-kaifangyuanzi.md\":\"DVani9MK\",\"posts_xdoctest-project.md\":\"D1YjEXRk\",\"posts_xdu-xjtu-os.md\":\"CF04UOFz\",\"posts_xian-event.md\":\"DSOIzACM\",\"posts_yanguohao-story.md\":\"Caxzgkds\",\"posts_yinfan-story.md\":\"BQD8IBpP\",\"posts_zhangyiqiao-story.md\":\"D5MUTeFE\",\"posts_zheng-story.md\":\"Brmo3hBv\",\"posts_zju-event.md\":\"CTI-BvwC\",\"posts_zju-se-os.md\":\"D4JHDafp\",\"posts_zuckerberg-letter-post.md\":\"CHMyePoL\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"zh-CN\",\"dir\":\"ltr\",\"title\":\"飞桨开源社区博客\",\"description\":\"Wonderful stories from PaddlePaddle contributors\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"postsPerPage\":10},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":true}");</script>
    
  </body>
</html>