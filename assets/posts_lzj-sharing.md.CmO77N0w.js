import{_ as i,c as a,m as t,o as n}from"./chunks/framework.CcfPirFf.js";const l="/assets/lzj-1.CmlvGcT4.png",r="/assets/lzj-2.hiiGXc-h.png",e="/assets/lzj-3.BuTI19yz.png",o="/assets/lzj-5.DyzQsDBe.png",h="/assets/lzj-6.BlxzrtAi.png",g="/assets/lzj-7.BOKSh--y.png",p="/assets/lzj-8.DmfMfrCB.png",k="/assets/lzj-9.D-XOGmqQ.png",u="/assets/lzj-10.CZ_E0xIn.png",d="/assets/lzj-11.npinW_DU.png",c="/assets/lzj-12.D9C1207d.png",E="/assets/lzj-13.TvDpS4zk.png",m="/assets/lzj-14.kHjGh_2z.png",b="/assets/lzj-15.CvIiq0Le.png",_="/assets/lzj-16.mcIag-4B.png",q="/assets/lzj-17.BUiQJQYy.png",F="/assets/lzj-18.zqLIW_jm.png",y="/assets/lzj-19.CufEdmI6.png",A=JSON.parse('{"title":"多模态推理模型开源相关工作综述","description":"","frontmatter":{"title":"多模态推理模型开源相关工作综述","date":"2025-03-11T00:00:00.000Z","author":{"name":"李志军","github":"ZhijunLStudio"},"category":"insights"},"headers":[],"relativePath":"posts/lzj-sharing.md","filePath":"posts/lzj-sharing.md"}'),f={name:"posts/lzj-sharing.md"};function C(P,s,v,B,R,x){return n(),a("div",null,[...s[0]||(s[0]=[t('<p>当前的多模态模型通常缺乏<strong>推理能力（reasoning）</strong>，为了提升其推理能力，研究主要集中在以下三个方向：</p><ul><li><strong>训练策略</strong>：保持模型架构不变，在后训练阶段采用 <strong>强化学习（如 GRPO/PPO）</strong> 进行优化，仅需普通的问答对数据，推理过程由模型自发生成。</li><li><strong>数据生成</strong>：构建高质量的推理数据集，并通过 <strong>监督微调（SFT）</strong> 训练模型。数据可借助 <strong>DeepSeek-R1</strong> 等推理模型进行蒸馏（仅蒸馏文本数据）。</li><li><strong>模型架构</strong>：优化多模态模型结构，例如将其中的 <strong>LLM 替换为具备推理能力的模型</strong>（如 <strong>DeepSeek-R1 蒸馏版</strong>）。</li></ul><p>目前，这些方向正在不断探索和优化，以提升多模态模型在数学推理、视觉问答等任务中的表现。 🚀</p><hr><h2 id="_1-训练策略" tabindex="-1">1. 训练策略 <a class="header-anchor" href="#_1-训练策略" aria-label="Permalink to &quot;1. 训练策略&quot;">​</a></h2><h3 id="_1-1-r1-v" tabindex="-1">1.1 R1-V <a class="header-anchor" href="#_1-1-r1-v" aria-label="Permalink to &quot;1.1 R1-V&quot;">​</a></h3><ul><li><strong>开源地址</strong>：<a href="https://github.com/Deep-Agent/R1-V" target="_blank" rel="noreferrer">GitHub - Deep-Agent/R1-V</a><br><img src="'+l+`" alt="Image"></li></ul><h4 id="_1-介绍" tabindex="-1">① 介绍 <a class="header-anchor" href="#_1-介绍" aria-label="Permalink to &quot;① 介绍&quot;">​</a></h4><p>R1-V 是多模态推理模型的早期代表之一。</p><ul><li><strong>训练环境</strong>：8 个 A100 GPU，时长 30 分钟，训练成本 2.62 USD（ 200 步）。</li></ul><h4 id="_2-训练流程" tabindex="-1">② 训练流程 <a class="header-anchor" href="#_2-训练流程" aria-label="Permalink to &quot;② 训练流程&quot;">​</a></h4><ul><li><p><strong>训练启动</strong>：使用 <code>torchrun</code> 运行 GRPO 脚本，并结合 <code>deepspeed</code>、<code>flash_attn</code>、<code>wandb</code> 等工具（代码结构简单）。</p></li><li><p><strong>核心流程</strong>：</p><ol><li><strong>初始化模型</strong>：加载主模型和参考模型（用于 KL 惩罚计算，确保主模型行为不偏离原始分布）。</li><li><strong>奖励函数</strong>： <ul><li><strong>准确率奖励</strong>：基于符号验证和字符串匹配计算答案正确性。</li><li><strong>格式奖励</strong>：确保输出格式符合 <code>&lt;think&gt;...&lt;/think&gt;&lt;answer&gt;...&lt;/answer&gt;</code> 结构。</li></ul></li><li><strong>损失计算</strong>： <ul><li>先用主模型生成 N 个不同的候选回答。</li><li>分别计算主模型与参考模型的 per-token 对数概率</li><li>计算 KL 散度</li><li>计算 Reward 与 Advantage</li><li>根据奖励函数得到每个样本的奖励，并对同一个 prompt 的多条生成做平均，用于优势函数的归一化</li><li>组合策略梯度与 KL 惩罚，得到最终的 loss</li></ul></li></ol></li><li><p><strong>System Prompt</strong>：</p><div class="language-plaintext vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">plaintext</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>A conversation between User and Assistant. The user asks a question, and the Assistant solves it.</span></span>
<span class="line"><span>The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.</span></span>
<span class="line"><span>The reasoning process and answer are enclosed within &lt;think&gt; &lt;/think&gt; and &lt;answer&gt; &lt;/answer&gt; tags, respectively.</span></span></code></pre></div></li></ul><p><img src="`+r+'" alt="Image"></p><h3 id="_3-关键注意点" tabindex="-1">③ 关键注意点 <a class="header-anchor" href="#_3-关键注意点" aria-label="Permalink to &quot;③ 关键注意点&quot;">​</a></h3><ul><li>训练时可使用 <strong>7 颗 GPU 进行训练，1 颗 GPU 运行 VLLM 计算 KL 损失</strong>。</li><li><strong>GRPO 训练仅需普通 QA 对，不需要 <code>think</code> 过程</strong>（如果有 <code>think</code> 数据，可先 SFT 再 GRPO）。</li></ul><hr><h3 id="_4-主要问题" tabindex="-1">④ 主要问题 <a class="header-anchor" href="#_4-主要问题" aria-label="Permalink to &quot;④ 主要问题&quot;">​</a></h3><ul><li><strong>奖励函数难以设置</strong>（当前代码中使用的是最简单的实现方案）。</li><li><strong>显存 OOM</strong>：详见 <a href="https://github.com/Deep-Agent/R1-V/issues/107" target="_blank" rel="noreferrer">Issue #107</a>。</li><li><strong>损失值经常为 0</strong>：详见 <a href="https://github.com/huggingface/open-r1/issues/239" target="_blank" rel="noreferrer">Huggingface Open-R1 Issue #239</a>。<br> 现在 GRPO 实现中策略都是单步更新，导致新旧策略是一样的，所以重要性采样系数是 1 ，然后优势函数 A 是一个组当中每个 reward 的标准化，那么对优势函数 A 求期望自然也就是 0 了。所以 GRPO 的 loss 实际上就是新旧策略的 KL 散度项再乘一个系数 beta ，这也就是为什么训练过程中 loss 曲线和 KL 散度曲线分布如此相似，因为只差了一个系数 beta 。</li></ul><hr><h3 id="_5-是否可以去掉-kl-约束" tabindex="-1">⑤ 是否可以去掉 KL 约束？ <a class="header-anchor" href="#_5-是否可以去掉-kl-约束" aria-label="Permalink to &quot;⑤ 是否可以去掉 KL 约束？&quot;">​</a></h3><p><img src="'+e+'" alt="Image"></p><p><strong>去除 KL 约束的影响：</strong></p><ul><li>不需要 <code>ref-model</code>，减少显存占用和前向计算成本。</li><li>可能减少训练不稳定性（类似于梯度裁剪）。</li><li>优化参数更加自由，可能探索到更佳的回答方案。</li></ul><p><strong>参考链接</strong></p><ul><li><a href="https://zhuanlan.zhihu.com/p/25067791857" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/25067791857</a></li></ul><hr><h2 id="_1-2-open-r1-multimodal" tabindex="-1">1.2 open-r1-multimodal <a class="header-anchor" href="#_1-2-open-r1-multimodal" aria-label="Permalink to &quot;1.2 open-r1-multimodal&quot;">​</a></h2><ul><li><strong>开源地址</strong>：<a href="https://github.com/EvolvingLMMs-Lab/open-r1-multimodal" target="_blank" rel="noreferrer">GitHub - EvolvingLMMs-Lab/open-r1-multimodal</a></li></ul><h3 id="_1-介绍-1" tabindex="-1">① 介绍 <a class="header-anchor" href="#_1-介绍-1" aria-label="Permalink to &quot;① 介绍&quot;">​</a></h3><ul><li><strong>开源了第一批专注于数学推理的 8K 多模态 RL 训练示例</strong>。</li><li><strong>数据来源</strong>：GPT-4o 生成（基于 Math360K 和 Geo170K），包含推理路径和可验证答案。</li><li><strong>采用 GRPO 训练策略</strong>，提升模型的数学推理能力。</li></ul><h3 id="_2-代码改进" tabindex="-1">② 代码改进 <a class="header-anchor" href="#_2-代码改进" aria-label="Permalink to &quot;② 代码改进&quot;">​</a></h3><ul><li><strong><code>format_reward</code> 正则匹配更加宽松</strong>。</li><li><strong><code>reward_processing_class</code> 计算奖励</strong>。</li><li><strong>生成方式优化</strong>： <ul><li><strong>R1-V</strong>：批量生成多个 completions。</li><li><strong>open-r1-multimodal</strong>：逐个生成再拼接（更灵活，但成本更高）。</li></ul></li></ul><p><img src="'+o+'" alt="Image"></p><hr><h3 id="_1-3-visualthinker-r1-zero" tabindex="-1">1.3 VisualThinker-R1-Zero <a class="header-anchor" href="#_1-3-visualthinker-r1-zero" aria-label="Permalink to &quot;1.3 VisualThinker-R1-Zero&quot;">​</a></h3><ul><li><strong>开源地址</strong>：<a href="https://github.com/turningpoint-ai/VisualThinker-R1-Zero" target="_blank" rel="noreferrer">GitHub - turningpoint-ai/VisualThinker-R1-Zero</a></li></ul><h4 id="_1-研究意义" tabindex="-1">① 研究意义 <a class="header-anchor" href="#_1-研究意义" aria-label="Permalink to &quot;① 研究意义&quot;">​</a></h4><ul><li><strong>首个在非 SFT 2B 模型上成功实现“顿悟时刻”（emergence）并增强多模态推理能力的开源代码</strong>。</li><li>通过强化学习（RL），模型在推理过程中出现了<strong>自我反思能力</strong>，类似于 DeepSeek-R1 论文中描述的现象。</li></ul><p><img src="'+h+'" alt="Image"></p><h4 id="_2-训练方法" tabindex="-1">② 训练方法 <a class="header-anchor" href="#_2-训练方法" aria-label="Permalink to &quot;② 训练方法&quot;">​</a></h4><ul><li><strong>基础模型</strong>：Qwen2-VL</li><li><strong>评测基准</strong>：CV-Bench</li><li><strong>训练数据</strong>：SAT 训练样本</li><li><strong>训练过程</strong>： <ul><li>初始阶段，模型倾向于生成 HTML 代码，导致推理长度下降。</li><li>经过强化学习优化后，模型逐渐采用更合理的输出格式，并且响应长度不断增加。</li><li>训练过程中，观察到一个<strong>多模态“顿悟时刻”</strong> —— 模型开始主动进行自我反思，并且<strong>响应长度与基准准确率呈正相关</strong>。</li></ul></li></ul><h4 id="_3-结论" tabindex="-1">③ 结论 <a class="header-anchor" href="#_3-结论" aria-label="Permalink to &quot;③ 结论&quot;">​</a></h4><ul><li><strong>更长的推理链可以极大提升基于视觉的任务表现</strong>。</li><li>在 <strong>无 SFT</strong> 的情况下，模型在 <strong>CV-Bench</strong> 上取得 <strong>59.47% 的准确率</strong>： <ul><li><strong>比基础模型高出约 30%</strong>。</li><li><strong>比 SFT 训练的模型高出约 2%</strong>。</li><li><strong>甚至超越了训练数据更多的指令微调模型</strong>。</li></ul></li></ul><p><img src="'+g+'" alt="Image"><img src="'+p+'" alt="Image"></p><h4 id="_4-代码改进" tabindex="-1">④ 代码改进 <a class="header-anchor" href="#_4-代码改进" aria-label="Permalink to &quot;④ 代码改进&quot;">​</a></h4><ul><li><strong>引入 <code>freeze_llm</code> 和 <code>freeze_vision</code>，控制训练过程中的参数更新</strong>。</li><li><strong>改进 <code>accuracy_reward</code></strong>，适配不同答案格式（如选择题）。</li><li><strong>增加 <code>length_reward</code></strong>，鼓励生成更长的回答，提高推理深度。</li><li><strong>兼容 <code>completions</code> 可能是字符串列表的情况，使代码更通用</strong>。</li></ul><p><img src="'+k+'" alt="Image"></p><h4 id="_5-其他发现" tabindex="-1">⑤ 其他发现 <a class="header-anchor" href="#_5-其他发现" aria-label="Permalink to &quot;⑤ 其他发现&quot;">​</a></h4><ul><li><strong>最大输入像素默认为 2,359,296</strong>，是 R1-V 默认值的 <strong>5.8 倍</strong>。</li><li><strong>最大响应长度默认为 8,192</strong>，是 R1-V 默认值的 <strong>16 倍</strong>。</li></ul><hr><h3 id="_1-4-efficient-r1-vllm" tabindex="-1">1.4 Efficient-R1-VLLM <a class="header-anchor" href="#_1-4-efficient-r1-vllm" aria-label="Permalink to &quot;1.4 Efficient-R1-VLLM&quot;">​</a></h3><ul><li><strong>开源地址</strong>：<a href="https://github.com/baibizhe/Efficient-R1-VLLM" target="_blank" rel="noreferrer">GitHub - baibizhe/Efficient-R1-VLLM</a></li></ul><h4 id="_1-介绍-2" tabindex="-1">① 介绍 <a class="header-anchor" href="#_1-介绍-2" aria-label="Permalink to &quot;① 介绍&quot;">​</a></h4><ul><li><strong>首个使用基于奖励的强化学习（GRPO）来微调 MoE（专家混合）视觉语言模型（DeepSeek2-VL）的项目</strong>。</li><li><strong>重点关注训练效率优化</strong>，采用 <strong>SGLang 加速推理</strong>，同时保持模型的推理能力。</li></ul><h4 id="_2-训练流程-pipeline" tabindex="-1">② 训练流程（Pipeline） <a class="header-anchor" href="#_2-训练流程-pipeline" aria-label="Permalink to &quot;② 训练流程（Pipeline）&quot;">​</a></h4><p>Vision-Language Input → DeepSeek2-VL MoE → GRPO Reward Optimization → Reasoning Output</p><ul><li>目前 <strong>等待开源</strong>，具体训练细节尚未公开。</li></ul><hr><h3 id="_1-5-vlm-r1" tabindex="-1">1.5 VLM-R1 <a class="header-anchor" href="#_1-5-vlm-r1" aria-label="Permalink to &quot;1.5 VLM-R1&quot;">​</a></h3><ul><li><strong>开源地址</strong>：<a href="https://github.com/om-ai-lab/VLM-R1" target="_blank" rel="noreferrer">GitHub - om-ai-lab/VLM-R1</a></li></ul><p><img src="'+u+'" alt="Image"></p><h4 id="_1-介绍-3" tabindex="-1">① 介绍 <a class="header-anchor" href="#_1-介绍-3" aria-label="Permalink to &quot;① 介绍&quot;">​</a></h4><ul><li><strong>适用于目标检测任务的多模态推理模型</strong>。</li><li><strong>在 R1-V 代码基础上进行改进</strong>，优化奖励函数以适应目标检测任务。</li></ul><h4 id="_2-代码改动" tabindex="-1">② 代码改动 <a class="header-anchor" href="#_2-代码改动" aria-label="Permalink to &quot;② 代码改动&quot;">​</a></h4><ul><li><strong>整体代码结构与 R1-V 类似</strong>，但主要的奖励函数不同： <ul><li><strong>R1-V 奖励函数</strong>：准确率奖励 + 格式奖励。</li><li><strong>VLM-R1 奖励函数</strong>：<strong>IOU（Intersection over Union）奖励 + 格式奖励</strong>。</li><li><strong>IOU 计算方式</strong>： <ul><li><strong>大于 0.5 的 IOU 计算为 1</strong>（成功匹配）。</li><li><strong>小于 0.5 的 IOU 计算为 0</strong>（失败）。</li></ul></li><li><strong>作者建议可能使用“软奖励”更合适，但仍需实验验证</strong>。</li></ul></li></ul><p><img src="'+d+'" alt="Image"></p><hr><h3 id="_1-6-visual-rft" tabindex="-1">1.6 Visual-RFT <a class="header-anchor" href="#_1-6-visual-rft" aria-label="Permalink to &quot;1.6 Visual-RFT&quot;">​</a></h3><ul><li><strong>开源地址</strong>：<a href="https://github.com/Liuziyu77/Visual-RFT" target="_blank" rel="noreferrer">GitHub - Liuziyu77/Visual-RFT</a></li><li><strong>论文地址</strong>：<a href="https://arxiv.org/html/2503.01785v1" target="_blank" rel="noreferrer">arXiv - Visual-RFT</a></li></ul><p><img src="'+c+'" alt="Image"></p><h4 id="_1-介绍-4" tabindex="-1">① 介绍 <a class="header-anchor" href="#_1-介绍-4" aria-label="Permalink to &quot;① 介绍&quot;">​</a></h4><ul><li><strong>基于强化学习优化的视觉推理模型</strong>，专注于提升多模态任务的推理能力。</li><li><strong>采用类 R1-V 训练架构， acc 奖励改为 iou + cls 奖励（对比 VLM-R1 为 iou 奖励）</strong>。</li></ul><p><img src="'+E+'" alt="Image"></p><h4 id="_2-示例" tabindex="-1">② 示例 <a class="header-anchor" href="#_2-示例" aria-label="Permalink to &quot;② 示例&quot;">​</a></h4><ul><li><strong>论文提供了多个示例，展示模型在不同视觉任务中的推理表现</strong>。</li></ul><p><img src="'+m+'" alt="Image"></p><hr><h2 id="_2-数据生成" tabindex="-1">2. 数据生成 <a class="header-anchor" href="#_2-数据生成" aria-label="Permalink to &quot;2. 数据生成&quot;">​</a></h2><h3 id="_2-1-r1-onevision" tabindex="-1">2.1 R1-Onevision <a class="header-anchor" href="#_2-1-r1-onevision" aria-label="Permalink to &quot;2.1 R1-Onevision&quot;">​</a></h3><ul><li><strong>开源地址</strong>：<a href="https://github.com/Fancy-MLLM/R1-Onevision" target="_blank" rel="noreferrer">GitHub - Fancy-MLLM/R1-Onevision</a></li><li><strong>团队</strong>：浙江大学</li><li><strong>特点</strong>： <ul><li><strong>长 COT（Chain of Thought）视觉数据集</strong>，结合 <strong>LLaVA-OneVision</strong> 高质量数据和特定领域数据（自然场景、数学、OCR、科学等）。</li><li><strong>数据处理流程</strong>： <ul><li><strong>数据清洗</strong> → <strong>图像标注</strong> → <strong>推理链生成</strong> → <strong>质量控制</strong> → <strong>规则强化学习</strong>。</li></ul></li></ul></li></ul><p><img src="'+b+'" alt="Image"></p><h4 id="_1-数据处理流程" tabindex="-1">① 数据处理流程 <a class="header-anchor" href="#_1-数据处理流程" aria-label="Permalink to &quot;① 数据处理流程&quot;">​</a></h4><p><img src="'+_+`" alt="Image"></p><h5 id="_1-数据准备与筛选" tabindex="-1">1. 数据准备与筛选 <a class="header-anchor" href="#_1-数据准备与筛选" aria-label="Permalink to &quot;1. 数据准备与筛选&quot;">​</a></h5><ul><li><strong>数据来源</strong>：整合多个数据集，包括 <strong>自然场景图像、OCR、图表、文本-图像对、数学、科学和逻辑推理数据</strong>，部分来自 LLaVA-OneVision 数据集。</li><li><strong>数据筛选</strong>：去除与推理任务无关的数据，仅保留 <strong>高质量、适用于推理的视觉数据</strong>。</li></ul><h5 id="_2-图像标注" tabindex="-1">2. 图像标注 <a class="header-anchor" href="#_2-图像标注" aria-label="Permalink to &quot;2. 图像标注&quot;">​</a></h5><p><strong>核心目标</strong>：将图像转换为正式语言描述，以便模型进行逻辑推理。</p><ul><li><strong>自然场景</strong>：使用 <strong>GPT-4o</strong> 生成详细描述，同时 <strong>Grounding DINO</strong> 提供目标检测信息。</li><li><strong>图表（Charts &amp; Diagrams）</strong>： <ul><li><strong>GPT-4o</strong> 将图表转换为代码（如 <strong>SPICE</strong> 处理电路图、<strong>PlantUML/Mermaid.js</strong> 生成流程图、<strong>HTML/CSV/JSON</strong> 处理表格、<strong>Matplotlib</strong> 生成数据图）。</li></ul></li><li><strong>文本图像</strong>： <ul><li>使用 <strong>EasyOCR</strong> 提取文本，GPT-4o 复原原始文档格式。</li></ul></li><li><strong>带文本内容的图像</strong>： <ul><li>结合 <strong>GPT-4o</strong> 生成详细描述、<strong>Grounding DINO</strong> 获取文本区域 bounding box、<strong>EasyOCR</strong> 提取文本，确保视觉与文本信息对齐。</li></ul></li><li><strong>数学图像</strong>： <ul><li><strong>GPT-4o</strong> 生成数学公式、推理步骤和最终答案的结构化描述。</li></ul></li></ul><h5 id="_3-推理链生成" tabindex="-1">3. 推理链生成 <a class="header-anchor" href="#_3-推理链生成" aria-label="Permalink to &quot;3. 推理链生成&quot;">​</a></h5><ul><li><strong>步骤</strong>： <ol><li>结合 <strong>图像的正式语言标注</strong> 和 <strong>相关问题</strong>，使用 <strong>DeepSeek-R1</strong> 生成初始推理链。</li><li><strong>角色扮演方法</strong>： <ul><li>让模型模拟视觉理解，反复查看图像，提取额外信息，反思并优化推理过程。</li><li>目标是让模型“像人一样”观察图像并思考，提高推理的准确性和上下文理解能力。</li></ul></li></ol></li></ul><h5 id="_4-质量控制" tabindex="-1">4. 质量控制 <a class="header-anchor" href="#_4-质量控制" aria-label="Permalink to &quot;4. 质量控制&quot;">​</a></h5><ul><li>使用 <strong>GPT-4o</strong> 进行过滤，确保推理链： <ul><li><strong>准确</strong>（符合事实和逻辑）。</li><li><strong>相关</strong>（与问题匹配）。</li><li><strong>一致</strong>（推理过程连贯）。</li></ul></li><li><strong>数据格式示例</strong>：<div class="language-json vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">   &quot;id&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&lt;unique_identifier&gt;&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">   &quot;image&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&lt;image_path&gt;&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">   &quot;gt&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&lt;ground_truth&gt;&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">   &quot;data_source&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&lt;dataset_source&gt;&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">   &quot;conversations&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">      { </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">&quot;from&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;human&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">&quot;value&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&lt;question&gt;&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">      { </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">&quot;from&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;assistant&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">&quot;value&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&lt;cot&gt;&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">   ],</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">   &quot;filter&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;valid or invalid&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">   &quot;judge_response&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&lt;gpt4o_filter_reason&gt;&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div></li></ul><h5 id="_5-基于规则的强化学习" tabindex="-1">5. 基于规则的强化学习 <a class="header-anchor" href="#_5-基于规则的强化学习" aria-label="Permalink to &quot;5. 基于规则的强化学习&quot;">​</a></h5><ul><li><strong>目标</strong>：在 <strong>SFT</strong> 后进一步优化推理的 <strong>结构化和准确性</strong>。</li></ul><h6 id="规则设计" tabindex="-1">规则设计 <a class="header-anchor" href="#规则设计" aria-label="Permalink to &quot;规则设计&quot;">​</a></h6><ol><li><strong>准确性规则</strong>： <ul><li>通过 <strong>正则表达式</strong> 提取模型的最终答案，并与 <code>ground truth</code> 进行数学验证。</li><li><strong>匹配则奖励，错误则惩罚</strong>。</li></ul></li><li><strong>格式规则</strong>： <ul><li>强制模型在推理过程中使用 <code>&lt;think&gt;</code> 和 <code>&lt;answer&gt;</code> 标签，并确保顺序正确。</li></ul><div class="language-plaintext vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">plaintext</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>&lt;think&gt; Step 1: Analyze the diagram... &lt;/think&gt;</span></span>
<span class="line"><span>&lt;think&gt; Step 2: Identify the key variables... &lt;/think&gt;</span></span>
<span class="line"><span>&lt;answer&gt; Final result: 42 &lt;/answer&gt;</span></span>
<span class="line"><span>- 使用 **正则表达式** 验证格式，并给予奖励或惩罚。</span></span></code></pre></div></li></ol><p><img src="`+q+`" alt="Image"></p><h5 id="_6-训练流程" tabindex="-1">6. 训练流程 <a class="header-anchor" href="#_6-训练流程" aria-label="Permalink to &quot;6. 训练流程&quot;">​</a></h5><ol><li><strong>监督微调（SFT）</strong>：基于 <strong>R1-Onevision Dataset</strong> 进行初步训练。</li><li><strong>强化学习（RL）</strong>： <ul><li>结合 <strong>Accuracy Rule</strong> 和 <strong>Formatting Rule</strong>，进行 <strong>奖励-惩罚机制训练</strong>。</li><li>目标是让模型在生成推理链和答案时 <strong>更精准、更结构化</strong>。</li></ul></li><li><strong>持续优化</strong>： <ul><li><strong>模型在训练过程中逐步改进推理能力</strong>，减少错误，提高一致性。</li></ul></li></ol><hr><h3 id="_2-2-deepseek-r1-vision" tabindex="-1">2.2 deepseek-r1-vision <a class="header-anchor" href="#_2-2-deepseek-r1-vision" aria-label="Permalink to &quot;2.2 deepseek-r1-vision&quot;">​</a></h3><ul><li><strong>开源地址</strong>：<a href="https://github.com/sungatetop/deepseek-r1-vision/tree/main" target="_blank" rel="noreferrer">GitHub - sungatetop/deepseek-r1-vision</a></li></ul><h4 id="_1-方法" tabindex="-1">① 方法 <a class="header-anchor" href="#_1-方法" aria-label="Permalink to &quot;① 方法&quot;">​</a></h4><ul><li><strong>将 SFT 数据转化为包含“思考”过程的内容</strong>，形成 <strong>COT（Chain of Thought）数据结构</strong>。</li><li><strong>使用 SFT 方法训练视觉模型</strong>，引导模型学习如何思考。</li><li><strong>开源了将原始问答对转换为思维链问答对的脚本（prompt）</strong>。</li></ul><h4 id="_2-数据转换" tabindex="-1">② 数据转换 <a class="header-anchor" href="#_2-数据转换" aria-label="Permalink to &quot;② 数据转换&quot;">​</a></h4><h6 id="原始的问答对" tabindex="-1">原始的问答对 <a class="header-anchor" href="#原始的问答对" aria-label="Permalink to &quot;原始的问答对&quot;">​</a></h6><div class="language-json vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">   {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;messages&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">         {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&lt;image&gt;尝试为这只猫拍照时可能会遇到哪些挑战？&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">         },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">         {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot; 当想要拍摄这只猫坐在木桌上直视前方时，可能会遇到几个挑战。首先，吸引猫的注意力并确保它专注于相机可能会很困难。猫通常很好奇，很容易被周围的环境分散注意力，因此保持猫的视线和兴趣可能会很具挑战性。其次，背景中的模糊图像可能会削弱主体或造成分散注意力的背景。关键是平衡良好的景深、照明和定位，从而创造出猫作为中心主题的视觉吸引力的照片。此外，调整相机设置，例如快门速度，以避免任何运动模糊，特别是如果猫准备扑击，有助于捕捉更清晰、更清晰的猫图像。&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;assistant&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">         }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">      ]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">   }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre></div><h6 id="处理后的问答对" tabindex="-1">处理后的问答对 <a class="header-anchor" href="#处理后的问答对" aria-label="Permalink to &quot;处理后的问答对&quot;">​</a></h6><div class="language-json vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">   &quot;messages&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">      {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">         &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">         &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&lt;image&gt;尝试为这只猫拍照时可能会遇到哪些挑战？&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">      },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">      {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">         &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;assistant&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">         &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&lt;think&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">嗯，我现在要帮用户分析拍摄这只猫时可能遇到的挑战。首先，我得仔细看看参考内容，里面提到了几个主要问题：吸引猫的注意力、背景干扰、快门速度设置。</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">好，先从吸引猫的注意力开始思考。猫通常很好奇，但也很容易被周围环境分散注意力。所以，拍摄的时候可能会很难让猫一直盯着相机。也许可以尝试用一些玩具或者零食来吸引它的注意力，但这样可能会影响构图，比如零食在旁边会让猫看向那里而不是镜头。</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">接下来是背景的问题。参考里提到背景中的模糊图像可能会削弱主体或者造成干扰。这让我想到，在拍摄时需要考虑背景是否简洁，是否有分散注意力的元素。如果背景太杂乱，即使主体拍得很好，整体效果也会受影响。可能需要用浅景深来虚化背景，但这样又得确保猫的眼睛是清晰的，否则整个照片就没意义了。</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">然后是快门速度的问题。猫的动作很快，尤其是当它们准备扑击的时候。如果快门速度不够高，可能会导致运动模糊，拍出来的照片就会不清晰。这时候需要调整相机设置，比如提高ISO或者使用更大的光圈来增加进光量，从而允许更快的快门速度。不过，这样可能会带来噪点问题，特别是在光线不足的情况下。</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">还有其他可能的问题吗？比如猫的好动性，它们不容易保持一个姿势太久，所以拍摄时间可能很短，需要快速捕捉。另外，光线也是一个因素，如果在自然光下拍摄，光线的变化可能会影响照片效果，尤其是在室内拍摄时，可能会有反光或者阴影问题。</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">总结一下，主要的挑战包括：吸引并保持猫的注意力、处理背景干扰、避免运动模糊、调整合适的快门速度和光圈设置，以及应对光线变化。这些都需要在实际拍摄中灵活应对，可能需要多次尝试才能拍到满意的照片。</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&lt;/think&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">当尝试为这只猫拍照时，可能会遇到以下挑战：</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">1. **吸引并保持猫的注意力**：猫通常好奇心强，但容易被周围环境分散注意力。要让猫专注于相机或某个方向，可能需要使用玩具、零食或其他引诱物来引导其视线。</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">2. **背景干扰**：背景中的模糊图像或杂乱元素可能会削弱主体（即猫）的表现力，导致照片显得不够专业或吸引人。因此，选择一个简洁或有吸引力的背景，并通过调整景深来虚化不必要的部分，是拍摄时需要注意的关键点。</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">3. **避免运动模糊**：猫的动作通常很快，尤其是在准备扑击时。使用较慢的快门速度可能会导致图像模糊。为了捕捉清晰的画面，可能需要提高快门速度，同时确保足够的光线进入相机以保持图像质量。</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">4. **光线条件**：在拍摄过程中，光线的变化可能会影响照片的效果。例如，在自然光下拍摄时，光线的方向和强度可能会改变猫的面部表情或整体氛围。使用反光板或闪光灯来补光，可以帮助平衡光线并突出猫的特征。</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">5. **猫的好动性**：猫不容易保持静止状态，尤其是在拍摄过程中。这可能需要摄影师具备快速反应的能力，以便在最佳时刻按下快门。</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">通过克服这些挑战，可以拍出一张既清晰又吸引人的猫的照片。&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">      }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">   ]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><hr><h2 id="_3-模型架构" tabindex="-1">3. 模型架构 <a class="header-anchor" href="#_3-模型架构" aria-label="Permalink to &quot;3. 模型架构&quot;">​</a></h2><h3 id="_3-1-align-ds-v" tabindex="-1">3.1 Align-DS-V <a class="header-anchor" href="#_3-1-align-ds-v" aria-label="Permalink to &quot;3.1 Align-DS-V&quot;">​</a></h3><ul><li><strong>开源地址</strong>：<a href="https://huggingface.co/PKU-Alignment/Align-DS-V" target="_blank" rel="noreferrer">Huggingface - PKU-Alignment/Align-DS-V</a></li><li><strong>团队</strong>：北京大学、香港科技大学</li></ul><h4 id="_1-介绍-5" tabindex="-1">① 介绍 <a class="header-anchor" href="#_1-介绍-5" aria-label="Permalink to &quot;① 介绍&quot;">​</a></h4><p>Align-DS-V 基于自研全模态对齐框架 <strong>Align-Anything</strong>，将 DeepSeek-R1 系列模型拓展至多模态推理能力，推出多模态推理模型 <strong>Align-DS-V</strong>。</p><h4 id="_2-方法" tabindex="-1">② 方法 <a class="header-anchor" href="#_2-方法" aria-label="Permalink to &quot;② 方法&quot;">​</a></h4><ul><li>Align-DS-V 通过对 <strong>视觉编码器</strong> 进行优化，使其能够更好地理解和处理图文混合的信息。</li><li>借鉴 <strong>LLaVA</strong> 训练思路，将 <strong>视觉信息投射到语言表示空间</strong>，实现图像与文本的深度结合。</li></ul><h4 id="_3-性能" tabindex="-1">③ 性能 <a class="header-anchor" href="#_3-性能" aria-label="Permalink to &quot;③ 性能&quot;">​</a></h4><ul><li><strong>多模态性能齐平 GPT-4o</strong>，并且推理能力不降反增。</li><li>通过 Align-Anything 框架，提升多模态任务的理解和推理能力。</li></ul><p><img src="`+F+'" alt="Image"></p><h4 id="_4-align-anything-框架架构" tabindex="-1">④ Align-Anything 框架架构 <a class="header-anchor" href="#_4-align-anything-框架架构" aria-label="Permalink to &quot;④ Align-Anything 框架架构&quot;">​</a></h4><p>Align-Anything 是一个通用的多模态对齐框架，可以用于优化不同模态（文本、图像、语音等）之间的融合能力。</p><p><img src="'+y+'" alt="Image"></p><h4 id="_5-意外发现" tabindex="-1">⑤ 意外发现 <a class="header-anchor" href="#_5-意外发现" aria-label="Permalink to &quot;⑤ 意外发现&quot;">​</a></h4><ul><li>将 <strong>DeepSeek-R1-Distill-Llama-8B</strong> 扩展到视觉模态后，Align-DS-R1 在原始文本模态推理能力上也取得了显著提高。</li></ul><h4 id="_6-开源贡献" tabindex="-1">⑥ 开源贡献 <a class="header-anchor" href="#_6-开源贡献" aria-label="Permalink to &quot;⑥ 开源贡献&quot;">​</a></h4><ul><li><strong>开源了模型</strong>：<a href="https://huggingface.co/PKU-Alignment/Align-DS-V" target="_blank" rel="noreferrer"><code>PKU-Alignment/Align-DS-V</code></a></li><li><strong>开源了多个数据集</strong>： <ul><li><a href="https://huggingface.co/datasets/PKU-Alignment/align-anything" target="_blank" rel="noreferrer"><code>PKU-Alignment/align-anything</code></a></li><li><a href="https://huggingface.co/datasets/PKU-Alignment/Align-Anything-Instruction-100K" target="_blank" rel="noreferrer"><code>PKU-Alignment/Align-Anything-Instruction-100K</code></a></li><li><a href="https://huggingface.co/datasets/PKU-Alignment/Align-Anything-TI2T-Instruction-100K" target="_blank" rel="noreferrer"><code>PKU-Alignment/Align-Anything-TI2T-Instruction-100K</code></a></li></ul></li></ul><blockquote><p><strong>注意</strong>：训练代码暂未开源。</p></blockquote><h2 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-label="Permalink to &quot;总结&quot;">​</a></h2><p>当前，多模态推理模型正朝着 <strong>强化学习优化推理能力、数据增强提升泛化能力、模型架构改进</strong> 三个关键方向发展：</p><h3 id="_1-强化学习-rlhf-grpo-优化推理能力" tabindex="-1">1. 强化学习（RLHF/GRPO）优化推理能力 <a class="header-anchor" href="#_1-强化学习-rlhf-grpo-优化推理能力" aria-label="Permalink to &quot;1. 强化学习（RLHF/GRPO）优化推理能力&quot;">​</a></h3><ul><li>采用 <strong>GRPO</strong> 等强化学习方法，对多模态推理能力进行优化。</li><li><strong>关键改进</strong>：通过奖励机制增强模型的推理链生成能力，提高逻辑推理的准确性和稳定性。</li><li><strong>代表性工作</strong>： <ul><li><strong><a href="https://github.com/Deep-Agent/R1-V" target="_blank" rel="noreferrer">R1-V</a></strong></li><li><strong><a href="https://github.com/EvolvingLMMs-Lab/open-r1-multimodal" target="_blank" rel="noreferrer">open-r1-multimodal</a></strong></li><li><strong><a href="https://github.com/turningpoint-ai/VisualThinker-R1-Zero" target="_blank" rel="noreferrer">VisualThinker-R1-Zero</a></strong></li></ul></li></ul><h3 id="_2-数据增强与高质量-cot-数据构造" tabindex="-1">2. 数据增强与高质量 COT 数据构造 <a class="header-anchor" href="#_2-数据增强与高质量-cot-数据构造" aria-label="Permalink to &quot;2. 数据增强与高质量 COT 数据构造&quot;">​</a></h3><ul><li>通过 <strong>GPT-4o、DeepSeek-R1 蒸馏</strong> 等方式构造 <strong>长 COT（Chain of Thought）数据</strong>，提升模型的推理深度和准确性。</li><li><strong>关键改进</strong>：采用结构化数据生成、多步推理链优化策略，提高模型的泛化能力。</li><li><strong>代表性工作</strong>： <ul><li><strong><a href="https://github.com/Fancy-MLLM/R1-Onevision" target="_blank" rel="noreferrer">R1-Onevision</a></strong></li><li><strong><a href="https://github.com/sungatetop/deepseek-r1-vision/tree/main" target="_blank" rel="noreferrer">deepseek-r1-vision</a></strong></li></ul></li></ul><h3 id="_3-多模态模型架构优化" tabindex="-1">3. 多模态模型架构优化 <a class="header-anchor" href="#_3-多模态模型架构优化" aria-label="Permalink to &quot;3. 多模态模型架构优化&quot;">​</a></h3><ul><li>采用 <strong>Align-Anything 框架</strong> 对视觉编码器进行优化，使语言模型能够更好地处理图文混合数据。</li><li><strong>关键改进</strong>：借鉴 <strong>LLaVA</strong> 训练思路，优化视觉信息与语言表示的融合能力。</li><li><strong>代表性工作</strong>： <ul><li><strong><a href="https://huggingface.co/PKU-Alignment/Align-DS-V" target="_blank" rel="noreferrer">Align-DS-V</a></strong></li></ul></li></ul><h2 id="关于自己" tabindex="-1">关于自己 <a class="header-anchor" href="#关于自己" aria-label="Permalink to &quot;关于自己&quot;">​</a></h2><p>我叫李志军，研究生毕业于中国农业大学。我的本科和研究生专业都不是计算机相关，但在读研期间，正好赶上了计算机视觉领域的快速发展，因此我对这个方向产生了浓厚兴趣，于是决定选择计算机视觉作为研究方向。</p><p>当时的 TensorFlow 还非常流行，但由于我导与飞桨有一些合作，我开始接触飞桨框架。我的研究课题是做苹果测产，目标是将算法部署到无人机上，实现果园的自动化测产。这期间使用了 PaddleDetection 和 PaddleLite，基于 PaddleDetection 对 YOLO 模型进行了魔改。虽然算法效果不错，但在将模型通过 PaddleLite 部署到树莓派时遇到了许多编译问题，尝试了很多办法也没解决。由于团队里大部分师兄和同学都专注于飞控相关领域，纯视觉方向的只有我一个人，缺乏技术支持，最终没能完成部署。不过，这段经历让我积累了不少经验，并基于魔改的 YOLO 算法发表了一篇论文，算是对自己的一个小交代吧。</p><p>2020 年底，我通过秋招加入了一家南京的清华系初创公司，起初以实习生的身份工作了大半年。毕业后顺利转正，并在这家公司工作了三年多，其中两年多担任算法部门负责人，主要负责计算机视觉方向的算法落地。算法落地的过程其实很“接地气”，但也很“苦逼”。需要频繁对接业务方，需求常常多变，最终的落地效果很大程度上依赖于业务方的配合程度。这段时间，我参与了许多项目，领域涵盖工业质检、安防等传统领域，也尝试过一些别的方向，比如国产化芯片适配（算能、瑞芯微等）、B 超机检测算法、K12 教育产品开发，以及一项有趣的奶牛面部识别算法。</p><p>在这些项目中，我的技术栈得到了很大的扩展。除了 Python 之外，还接触了 C++/C# 等语言，同时对计算机视觉的各类任务有了更深入的理解和实践，包括目标检测、图像分类、图像分割、深度估计、视频理解、少样本学习等。</p><p>2024 年下半年，我决定切换赛道，加入了现在的创新中心，主要聚焦于 AI 和芯片的结合应用。目前的工作方向是多模态技术，针对电路图进行特定信息的提取。这是一个全新的领域，充满挑战，但也让我有了更深的技术积累。</p><p>在 2024 年底，我还参加了开放原子基金会与 PaddleMix 举办的一场比赛，主要解决多模态数据流问题。很幸运，这次比赛拿到了第一名，也算是对自己技术能力的一次小小认可。</p><p>总的来说，这些年的经历让我在技术积累和项目经验上有了较全面的提升，也让我深刻体会到，技术是一方面，业务落地和跨团队协作同样重要。未来，我希望能够在技术深耕的同时，能和飞桨一起，找到更多技术与实际场景结合的机会，创造更大的价值。</p>',144)])])}const V=i(f,[["render",C]]);export{A as __pageData,V as default};
